unnest_tokens(word, V2)
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
# N <- 2373
# tokens1 <- word_tokenizer(tolower(data1$V2[1:N]))
#set random seed for reproducability
set.seed(445)
N <- 2373
View(cleanedData)
View(tidyData)
#installing packages required for the assignment in the background of the rmarkdown
r <- getOption("repos")
r["CRAN"] <- "https://cran.curtin.edu.au/"
options(repos = r)
if(!require(clusterGeneration)){
install.packages("clusterGeneration")
}
if(!require(mvtnorm)){
install.packages("mvtnorm")
}
if(!require(reshape2)){
install.packages("reshape2")
}
if(!require(text2vec)){
install.packages("text2vec")
}
if(!require(tidytext)){
install.packages("tidytext")
}
if(!require(tokenizers)){
install.packages("tokenizers")
}
if(!require(stopwords)){
install.packages("stopwords")
}
library(text2vec)
library(tidytext)
library(dplyr)
library(tokenizers)
library(stopwords)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
data$V2 <- as.character(data$V2)
tokens <- word_tokenizer(tolower(data$V2[1:N]))
tidyData <- data %>%
unnest_tokens(word, V2)
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
lapply(cleanedData, count)
summary(cleanedData)
# N <- 2373
# tokens1 <- word_tokenizer(tolower(data1$V2[1:N]))
#set random seed for reproducability
set.seed(445)
N <- 2373
summary(cleanedData$V1)
# N <- 2373
# tokens1 <- word_tokenizer(tolower(data1$V2[1:N]))
#set random seed for reproducability
set.seed(445)
N <- 2373
summary(cleanedData$word)
# N <- 2373
# tokens1 <- word_tokenizer(tolower(data1$V2[1:N]))
#set random seed for reproducability
set.seed(445)
N <- 2373
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
count(V1, word, sort = TRUE)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
count(cleanedData$V1, word, sort = TRUE)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(V1, word, sort = TRUE)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(data$V1, word, sort = TRUE)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
group_by(V1)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
group_by(data$V1)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(data$V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(tidyData$V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V2 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(tidyData$V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.character(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(tidyData$V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(tidyData$V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
count(cleanedData$V1, word)
count(word)
count(cleandData$word)
count(cleaneddData$word)
count(cleanedData$word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
count(tidyData$word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2) %>%
count(word)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords())
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = True)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, wordm sort = TRUE)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = TRUE)
#installing packages required for the assignment in the background of the rmarkdown
r <- getOption("repos")
r["CRAN"] <- "https://cran.curtin.edu.au/"
options(repos = r)
if(!require(clusterGeneration)){
install.packages("clusterGeneration")
}
if(!require(mvtnorm)){
install.packages("mvtnorm")
}
if(!require(reshape2)){
install.packages("reshape2")
}
if(!require(text2vec)){
install.packages("text2vec")
}
if(!require(tidytext)){
install.packages("tidytext")
}
if(!require(tokenizers)){
install.packages("tokenizers")
}
if(!require(stopwords)){
install.packages("stopwords")
}
if(!require(ggplot2)){
install.packages("ggplot2")
}
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = TRUE)
total_words <- cleanedData %>%
group_by(V1) %>%
summarise(total = sum(n))
cleanedData <- left_join(cleanedData, total_words)
ggplot(cleanedDate, aes(n/total, fill = V1)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~V1, ncol = 2, scales = "free_y")
ggplot(cleanedData, aes(n/total, fill = V1)) +
geom_histogram(show.legend = FALSE) +
xlim(NA, 0.0009) +
facet_wrap(~V1, ncol = 2, scales = "free_y")
#set random seed for reproducability
set.seed(445)
N <- 2373
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set and calculate the count for each word
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = TRUE)
#calculate the total amount of words for each catagory
total_words <- cleanedData %>%
group_by(V1) %>%
summarise(total = sum(n))
#create dataset for wordcount by joining cleanedData and total_words
data1 <- left_join(cleanedData, total_words)
#TF-IDF addition to dataset
data1 <- data1 %>%
bind_tf_idf(word, V1, n)
View(data1)
View(data1)
# Set the parameters:
set.seed(12345) # save the random seed to make the results reproducble
N <- 1000 # number of samples
K <- 3    # number of clusters
D <- 2    # number of dimensions
# Initializations:
Phi <- runif(K); Phi <- Phi/sum(Phi)    # Phi(k) indicates the fraction of samples that are from cluster k
Nk <- matrix(0,nrow = K)    # initiate  the effective number of points assigned to each cluster
Mu <- matrix(runif(K*D, min=-1)*10,nrow = K, ncol = D)    # initiate the centriods (means) of the clusters (randomly chosen)
Sigma <- matrix(0,nrow = K, ncol = D^2)    # initiate the covariance matrix
View(Mu)
View(Nk)
View(Sigma)
View(Sigma)
View(Mu)
View(Mu)
View(Nk)
View(Mu)
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set and calculate the count for each word
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = TRUE)
#calculate the total amount of words for each catagory
total_words <- cleanedData %>%
group_by(V1) %>%
summarise(total = sum(n))
#create dataset for wordcount by joining cleanedData and total_words
data1 <- left_join(cleanedData, total_words)
#TF-IDF addition to dataset
data1 <- data1 %>%
bind_tf_idf(word, V1, n)
View(Mu)
#set random seed for reproducability
set.seed(445)
N <- 49977
K <- 3
D <- 2
# Initializations:
Phi <- runif(K); Phi <- Phi/sum(Phi)    # Phi(k) indicates the fraction of samples that are from cluster k
Nk <- matrix(0,nrow = K)    # initiate  the effective number of points assigned to each cluster
Mu <- matrix(runif(K*D, min=-1)*10,nrow = K, ncol = D)    # initiate the centriods (means) of the clusters (randomly chosen)
Sigma <- matrix(0,nrow = K, ncol = D^2)    # initiate the covariance matrix
View(Mu)
#set random seed for reproducability
set.seed(445)
N <- 49977
K <- 4
D <- 2
# Initializations:
Phi <- runif(K); Phi <- Phi/sum(Phi)    # Phi(k) indicates the fraction of samples that are from cluster k
Nk <- matrix(0,nrow = K)    # initiate  the effective number of points assigned to each cluster
Mu <- matrix(runif(K*D, min=-1)*10,nrow = K, ncol = D)    # initiate the centriods (means) of the clusters (randomly chosen)
Sigma <- matrix(0,nrow = K, ncol = D^2)    # initiate the covariance matrix
View(Mu)
View(Nk)
View(Sigma)
View(Sigma)
View(Sigma)
View(Sigma)
# Create the covariance matrices:
for (k in 1:K){
# For each cluster generate one sigma matrix
Sigma[k,] <- genPositiveDefMat(D)$Sigma[1:D^2]
}
ggplot(data = data1, aes(x = as.numeric(word), y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
data1$word <- as.numeric(data1$word)
ggplot(data = data1, aes(x = as.numeric(word), y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
View(data1)
unique(tolower(words))
#read in the data needed for analysis.
data <- read.delim("D:/UNI/data_science/fit5201/mywork/uni_fit5201/assignment_2/A2/q1/20ng-train-all-terms.txt", header=FALSE, row.names=NULL)
#make V2 column a char data type instead of factor
data$V2 <- as.character(data$V2)
data$V1 <- as.factor(data$V1)
#get tokens from data and give each token a new row
tidyData <- data %>%
unnest_tokens(word, V2)
#remove stopwords from data set and calculate the count for each word
cleanedData  <- tidyData %>%
anti_join(get_stopwords()) %>%
count(V1, word, sort = TRUE)
#calculate the total amount of words for each catagory
total_words <- cleanedData %>%
group_by(V1) %>%
summarise(total = sum(n))
#create dataset for wordcount by joining cleanedData and total_words
data1 <- left_join(cleanedData, total_words)
#TF-IDF addition to dataset
data1 <- data1 %>%
bind_tf_idf(word, V1, n)
#set random seed for reproducability
set.seed(445)
N <- 49977
K <- 4
D <- 2
# Initializations:
Phi <- runif(K); Phi <- Phi/sum(Phi)    # Phi(k) indicates the fraction of samples that are from cluster k
Nk <- matrix(0,nrow = K)    # initiate  the effective number of points assigned to each cluster
Mu <- matrix(runif(K*D, min=-1)*10,nrow = K, ncol = D)    # initiate the centriods (means) of the clusters (randomly chosen)
Sigma <- matrix(0,nrow = K, ncol = D^2)    # initiate the covariance matrix
# Create the covariance matrices:
for (k in 1:K){
# For each cluster generate one sigma matrix
Sigma[k,] <- genPositiveDefMat(D)$Sigma[1:D^2]
}
unique(tolower(words))
View(data1)
unique(tolower(word))
unique(tolower(data1$word))
ggplot(data = data1, aes(x = word, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
x< - unique(tolower(data1$word))
x < - unique(tolower(data1$word))
x <- unique(tolower(data1$word))
ggplot(data = data1, aes(x = x, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
ggplot(data = data1, aes(x = tf, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
ggplot(data = data1, aes(x = total, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
ggplot(data = data1, aes(x = idf, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
ggplot(data = data1, aes(x = tf_idf, y = n, colour = factor(V1))) +
geom_point() +
scale_colour_discrete(guide = guide_legend(title = "Cluster")) +
ggtitle('Dataset') + theme_minimal()
doc1 <- data1[data1$V1 == "	sci.crypt"]
doc1 <- data1[which(data1$V1 == "sci.crypt"),]
View(doc1)
doc1 <- data1[which(data1$V1 == "sci.crypt"),]
doc2 <- data1[which(data1$V1 == "sci.electronics"),]
doc3 <- data1[which(data1$V1 == "sci.space"),]
doc4 <- data1[which(data1$V1 == "sci.med"),]


---
title: "Activity 1.1 K-Nearest Neighbour Classifier"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---


### Background
In this activity, we learn how [K-Nearest Neighbors (KNN)](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) classifier works. KNN is a simple non-parametric model (ironically non-parametric here means infinite number of parameters), which is an example of [instance-based](https://en.wikipedia.org/wiki/Instance-based_learning) supervised learning. We will use KNN as a vehicle to practice some of the basic concepts of machine learning. KNN is a [lazy learner](https://en.wikipedia.org/wiki/Lazy_learning) that stores all training data points and their labels in memory, and predict the class label for a new data point based on its similarity to the training data (in fact the stored training data points can be considered as parameters).

Consider a training dataset containing $(x,t)$ pairs where $x$ is the input and $t$ is the target class label. Suppose we are given a similarity measure $sim(x_1,x_2)$ which gives the similarity score when fed with two data points. Given a test data point $x$, the K-nearest neighbour classifier works as follows:

* Select the top K most similar data points to x from the training set</li>
*	Look at the label of the K-nearest neighbours and select the label which has the majority vote.</li>

If the classes are equally common among the neighbours (e.g., two positive and two negative neighbours in binary classification when K=4), the test datapoint is randomly assigned to one of the classes. For example, Figure <strong>A.1</strong> (below) illustrates such situation where the test datapoint (shown by <span style="color: #00ff00;">green</span>) has exactly two neighbours from each class (marked by <span style="color: #ff0000;">red</span> and <span style="color: #3366ff;">blue</span>).

<a href="http://www.saedsayad.com/k_nearest_neighbors.htm" rel="attachment wp-att-92100"><img class="wp-image-92100 size-full" src="https://www.alexandriarepository.org/wp-content/uploads/20160413152921/A.1.png" alt="Figure A.1: KNN for Classification. The green dot indicates a sample with an unknown class label, while red and blue samples are training observation from default and non-default classes, respectively. Source: http://www.saedsayad.com/k_nearest_neighbors.htm" width="497" height="274" /></a> 

Figure A.1: KNN for Classification. The green dot indicates a sample with an unknown class label, while red and blue samples are training observation from default and non-default classes, respectively. Source: http://www.saedsayad.com/k_nearest_neighbors.htm


# Steps for Activity 1
1. Load the iris dataset and divide it to separate training and testing sets,
2. Define a function that calculates the majority vote,
3. Define KNN function that takes training labeled samples, testing samples, $K$ and a distance metric and predicts the class labels for the testing samples,
4. Apply KNN where for some values of $K$ and report training and testing error
5. Plot training and testing error versus $1/K$ where $K \in \{1, ..., 100\}$

# Implementation of the Above Steps
Here, we implement a basic KNN classifier. In this task, we use a simple, yet very popular, dataset to investigate the performance of our KNN. 

### Load and Explor Data
Let us start with loading the libraries and dataset.

```{r}
install.packages("reshape2")
install.packages("ggplot2")
install.packages("corrplot")
```
```{r}
library(reshape2)
library(ggplot2)
library(corrplot)
# Load data: it's built in to R, however, you can also get it online
# iris <- read.csv(url("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"), header = FALSE)
library(datasets)
```

```{r}
data(iris)
# take a look at the data
head(iris)
# Shown are 4 measurements (petal & sepal width & length) for 3 species of iris flowers, where sepal is: 
# "One of the usually separate, green parts that surround and protect the flower bud" (or petals)
```


```{r}
dim(iris) # 150 x 5 records
```

```{r}
install.packages("Cairo")
#a few visualizations wont hurt!
```

```{r}
## the followin plot illustrates petal measurments:
ggplot(data=iris, aes(x=Petal.Length, y=Petal.Width, color=Species)) + 
    geom_point() + geom_rug()+ theme_minimal() + ggtitle("Petal Measurements")
## and this one shows the correlation between the features (input variables)
#corrplot.mixed(cor(iris[,-5]), lower="ellipse", upper="number")
```

### Training and Testing Sets

```{r}
# set random seed
set.seed(1234)
# permute iris, shuffle or mix them up
iris <- iris[sample(1:nrow(iris),nrow(iris)),]
# create  training and testing subsets:
train.index = 1:105
train.data <- iris[train.index, -5] # grab the first 100 records, leave out the species (last column)
train.label <- iris[train.index, 5]
test.data <- iris[-train.index, -5] # grab the last 50 records, leave out the species (last column)
test.label <- iris[-train.index, 5]

dim(train.data) # 100 records
dim(test.data) # 50 records
```

```{r}
head(iris) # the shuffled records
```

```{r}
head(train.data) # the first 100 records without the Species
```

### Majority Vote

```{r}
# define an auxiliary function that calculates the majority votes (or mode!)
majority <- function(x) {
   uniqx <- unique(x)
   uniqx[which.max(tabulate(match(x, uniqx)))]
}
```

### KNN Classifier

```{r}
# KNN function (distance should be one of euclidean, maximum, manhattan, canberra, binary or minkowski)
knn1 <- function(train.data, train.label, test.data, K=3, distance = 'euclidean'){
    ## count number of train samples
    train.len <- nrow(train.data)
    
    ## count number of test samples
    test.len <- nrow(test.data)
    
    ## calculate distances between samples
    dist <- as.matrix(dist(rbind(test.data, train.data), method= distance))[1:test.len, (test.len+1):(test.len+train.len)]
    
    ## for each test sample...
    for (i in 1:test.len){
        ### ...find its K nearest neighbours from training sampels...
        nn <- as.data.frame(sort(dist[i,], index.return = TRUE))[1:K,2]
        
        ###... and calculate the predicted labels according to the majority vote
        test.label[i]<- (majority(train.label[nn]))
    }
    
    ## return the class labels as output
    return (test.label)
}
```


```{r}
# let see what is the prediciton of our knn for test samples when K=4
knn1(train.data, train.label, test.data, K=4)
```

```{r}
# and a confusion matrix for K = 5
prop.table(table(knn1(train.data, train.label, test.data, K=5), test.label))*100
```

```{r}
# calculate the train and test missclassification rates for K in 1:100 
# THIS MAY TAKE A FEW MINUTES TO COMPLETE!
miss <- data.frame('K'=1:6, 'train'=rep(0,6), 'test'=rep(0,6))
for (k in 1:6){
    miss[k,'train'] <- sum(knn1(train.data, train.label, train.data, K=k) != train.label)/nrow(train.data)*100
    miss[k,'test'] <-  sum(knn1(train.data, train.label, test.data, K=k)  != test.label)/nrow(test.data)*100
}
```


```{r}
# plot misclassification percentage for train and test data sets
miss.m <- melt(miss, id='K') # reshape for visualization
names(miss.m) <- c('K', 'type', 'error')
ggplot(data=miss.m, aes(x=log(1/K), y=error, color=type)) + geom_line() +
       scale_color_discrete(guide = guide_legend(title = NULL)) + theme_minimal() +
       ggtitle("Missclassification Error")
```


# Discussion

1. As $K$ increases, does the complexity of the KNN classifier increase or decrease?
2. What is the relationship between $1/K$ and the training error?
3. What is the relationship between $1/K$ and the testing error?
4. How do you explain the difference between training and testing error trends as the complexity of the KNN classifier increases?
5. Can you tell me the areas where the model overfits and underfits? What is the best value for $K$?




